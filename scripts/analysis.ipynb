{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab3f764-6406-4c6c-99f6-ae41879327ae",
      "metadata": {
        "id": "5ab3f764-6406-4c6c-99f6-ae41879327ae"
      },
      "outputs": [],
      "source": [
        "# Import all necesary libraries\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_df(df, filename):\n",
        "    filepath = f'./{filename}.csv'\n",
        "\n",
        "    if os.path.exists(filepath):\n",
        "        os.remove(filepath)\n",
        "\n",
        "    df.to_csv(filepath, index=False)"
      ],
      "metadata": {
        "id": "m3lUV9ElTucE"
      },
      "id": "m3lUV9ElTucE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note\n",
        "I am providing the data as a tar file because I am developing this in google colab and it is easier to untar the data for use, this is all of the data I collected in PolicyMap"
      ],
      "metadata": {
        "id": "CNhuSvNxkAv_"
      },
      "id": "CNhuSvNxkAv_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Untar data folder\n",
        "import tarfile\n",
        "data_tar_path = './data.tar.gz'\n",
        "\n",
        "# Set base directory for data folder\n",
        "data_dir = './data/'\n",
        "\n",
        "if os.path.exists(data_tar_path):\n",
        "    with tarfile.open(data_tar_path, 'r:gz') as tar:\n",
        "        tar.extractall('./')"
      ],
      "metadata": {
        "id": "q4pY56ExjitR"
      },
      "id": "q4pY56ExjitR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7bc5b0ab-1b28-4967-97c1-e72414b13ffb",
      "metadata": {
        "id": "7bc5b0ab-1b28-4967-97c1-e72414b13ffb"
      },
      "source": [
        "# Data ingestion and cleaning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def combine_csvs(data_dir='data'):\n",
        "  csv_files = glob.glob(os.path.join(data_dir, '**', '*.csv'), recursive=True)\n",
        "\n",
        "  dfs = []\n",
        "\n",
        "  for csv_file in csv_files:\n",
        "    # Read the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Check if df is empty before proceeding\n",
        "    if df.empty:\n",
        "        print(f\"Warning: {csv_file} is empty. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    first_col = df.columns[0]\n",
        "\n",
        "    # Check if first_col exists in df before using it for filtering\n",
        "    if first_col not in df.columns:\n",
        "        print(f\"Warning: First column '{first_col}' not found in {csv_file}. Skipping filter.\")\n",
        "    else:\n",
        "      # Make sure 'GeoID' is consistent, case-insensitive and stripped\n",
        "      # Check if 'GeoID' is actually a column header before trying to filter by it\n",
        "      if 'GeoID' in df.columns or 'geoid' in df.columns:\n",
        "         df = df[~df[first_col].astype(str).str.strip().str.lower().isin([first_col.lower(), 'geoid'])]\n",
        "         df = df.reset_index(drop=True)\n",
        "\n",
        "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "    # filename = os.path.basename(csv_file)\n",
        "    # df['source_file'] = os.path.splitext(filename)[0]\n",
        "\n",
        "    dfs.append(df)\n",
        "\n",
        "  if not dfs:\n",
        "    print(\"No CSV files found to concatenate.\")\n",
        "    return pd.DataFrame() # Return an empty DataFrame if no files were found\n",
        "\n",
        "  combined_df = pd.concat(dfs, ignore_index=True)\n",
        "  return combined_df"
      ],
      "metadata": {
        "id": "w66m4fUKI1sf"
      },
      "id": "w66m4fUKI1sf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = combine_csvs(data_dir)\n",
        "\n",
        "# Header row is duplicated so remove the second one\n",
        "df = df[df['geoid'] != 'GeoID']\n",
        "\n",
        "# Clean up column names and drop redundant or not needed columns\n",
        "columns_to_remove = ['geography_type_description',\n",
        "                     'geography_name',\n",
        "                     'formatted_geoid',\n",
        "                     'geographic_vintage',\n",
        "                     'data_source',\n",
        "                     'selected_location',\n",
        "                     'data_time_period']\n",
        "\n",
        "df.drop(columns=columns_to_remove, inplace=True)\n",
        "df['year'] = 2021\n",
        "\n",
        "columns_renames = {\n",
        "    'percent_of_people_who_drove_to_work': 'pct_drove_to_work',\n",
        "    'avg._vehicles_per_household': 'avg_vehicles_per_household',\n",
        "    'pct._of_people_walking_to_work': 'pct_walking_to_work',\n",
        "    'pct._of_people_who_took_public_transit_to_work': 'pct_transit_to_work',\n",
        "    'pct._of_people_who_rode_bike_to_work': 'pct_bike_to_work',\n",
        "    'pedestrian-oriented_road_density': 'pedestrian_oriented_road_density',\n",
        "    'national_walkability_index': 'national_walkability_index',\n",
        "    'sits_in_state': 'state'\n",
        "}\n",
        "\n",
        "# These are to be more friendly for PowerBI\n",
        "df = df.rename(columns={\n",
        "    'jobs_within_45_minutes_auto_travel_time':'jobs_45m_auto',\n",
        "    'pedestrian_oriented_road_density':'ped_oriented_rd_dens',\n",
        "    'national_walkability_index':'walk_idx'\n",
        "})\n",
        "\n",
        "df.rename(columns=columns_renames, inplace=True)"
      ],
      "metadata": {
        "id": "rMIODyY2UQ_N"
      },
      "id": "rMIODyY2UQ_N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agg_dict = {\n",
        "    'state': 'first',\n",
        "    'distance_to_transit': 'first',\n",
        "    'jobs_45m_auto': 'first',\n",
        "    'walk_idx': 'first',\n",
        "    'road_network_density': 'first',\n",
        "    'pedestrian_oriented_road_density': 'first',\n",
        "    'avg_vehicles_per_household': 'first',\n",
        "    'pct_drove_to_work': 'first',\n",
        "    'pct_walking_to_work': 'first',\n",
        "    'pct_transit_to_work': 'first',\n",
        "    'population_density': 'first',\n",
        "    'per_capita_income': 'first',\n",
        "    'year': 'first'\n",
        "}\n",
        "\n",
        "numeric_cols = [\n",
        "    'distance_to_transit',\n",
        "    'jobs_45m_auto',\n",
        "    'walk_idx',\n",
        "    'road_network_density',\n",
        "    'pedestrian_oriented_road_density',\n",
        "    'avg_vehicles_per_household',\n",
        "    'pct_drove_to_work',\n",
        "    'pct_walking_to_work',\n",
        "    'pct_transit_to_work',\n",
        "    'population_density',\n",
        "    'per_capita_income',\n",
        "]\n",
        "\n",
        "# Apply aggregation\n",
        "df_consolidated = df.groupby('geoid', as_index=False).agg(agg_dict)\n",
        "\n",
        "assert df_consolidated['geoid'].nunique() == len(df_consolidated), \"Duplicate geoids found\"\n",
        "\n",
        "df = df_consolidated\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "CvsPFf7-bZE3"
      },
      "id": "CvsPFf7-bZE3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numeric_cols:\n",
        "    # Convert the column to numeric, coercing errors to NaN\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    # Impute missing values with the median of the state group\n",
        "    df[col] = df.groupby('state')[col].transform(lambda x: x.fillna(x.median()))\n",
        "\n",
        "# It is easier for me to analyize vehicles per household if it is a whole number\n",
        "df['avg_vehicles_per_household'] = df['avg_vehicles_per_household'].astype(int)"
      ],
      "metadata": {
        "id": "XgHJDddP5Qwc"
      },
      "id": "XgHJDddP5Qwc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the numeric columns for correlation analysis\n",
        "numeric_df = df.select_dtypes(include=np.number)\n",
        "\n",
        "# Drop the 'year' column as it has no correlation with other columns\n",
        "numeric_df = numeric_df.drop(columns=['year'])\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = numeric_df.corr()\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=1)\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gkopbSwt3Bge"
      },
      "id": "gkopbSwt3Bge",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba9446d7"
      },
      "source": [
        "# Create quartiles for walkability index and per capita income\n",
        "df['walkability_quartile'] = pd.qcut(df['walk_idx'], q=4, labels=False, duplicates='drop')\n",
        "df['affordability_quartile'] = pd.qcut(df['per_capita_income'], q=4, labels=False, duplicates='drop')\n",
        "\n",
        "# Combine quartiles to create a composite index (example: summing quartiles)\n",
        "# You might want to weight these differently or use a different combination method\n",
        "df['walkability_affordability_index'] = df['walkability_quartile'] + df['affordability_quartile']\n",
        "\n",
        "display(df[['walk_idx', 'walkability_quartile', 'per_capita_income', 'affordability_quartile', 'walkability_affordability_index']].head())\n",
        "save_df(df, 'rev5')"
      ],
      "id": "ba9446d7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "features = ['population_density']\n",
        "target = 'distance_to_transit'\n",
        "\n",
        "# Drop rows with missing values in the selected features and target\n",
        "df_cleaned = df.dropna(subset=features + [target])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X = df_cleaned[features]\n",
        "y = df_cleaned[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a simple linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"Root Mean Squared Error: {rmse}\")\n",
        "\n",
        "# You can also print the model coefficients\n",
        "print(f\"Coefficient: {model.coef_[0]}\")\n",
        "print(f\"Intercept: {model.intercept_}\")"
      ],
      "metadata": {
        "id": "xRfE7d9hcEur"
      },
      "id": "xRfE7d9hcEur",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}